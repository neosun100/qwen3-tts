import os, io, time, asyncio, logging, json
from typing import Optional, List
from contextlib import asynccontextmanager
from dataclasses import asdict

import torch
import numpy as np
import soundfile as sf
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse, JSONResponse, HTMLResponse
from pydantic import BaseModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("qwen3-tts")

PORT = int(os.getenv("PORT", 8766))
GPU_IDLE_TIMEOUT = int(os.getenv("GPU_IDLE_TIMEOUT", 600))
CUDA_DEVICE = os.getenv("CUDA_DEVICE", "cuda:0")
OUTPUT_DIR = "/tmp/qwen3-tts"
os.makedirs(OUTPUT_DIR, exist_ok=True)

SPEAKERS = ["Vivian", "Serena", "Uncle_Fu", "Dylan", "Eric", "Ryan", "Aiden", "Ono_Anna", "Sohee"]
LANGUAGES = ["Auto", "Chinese", "English", "Japanese", "Korean", "German", "French", "Russian", "Portuguese", "Spanish", "Italian"]

_MODEL_DIR = os.getenv("QWEN_TTS_MODEL_DIR", "/app/models")
def _model_path(name):
    local = os.path.join(_MODEL_DIR, name)
    return local if os.path.isdir(local) else f"Qwen/{name}"

MODEL_MAP = {
    "custom_voice": _model_path("Qwen3-TTS-12Hz-1.7B-CustomVoice"),
    "voice_design": _model_path("Qwen3-TTS-12Hz-1.7B-VoiceDesign"),
    "voice_clone": _model_path("Qwen3-TTS-12Hz-1.7B-Base"),
}
TOKENIZER_PATH = _model_path("Qwen3-TTS-Tokenizer-12Hz")

SPEAKER_INFO = {
    "Vivian":   {"desc_en": "Bright, slightly edgy young female", "desc_zh": "æ˜Žäº®ã€ç•¥å¸¦é”åˆ©çš„å¹´è½»å¥³å£°", "native": "Chinese", "gender": "Female"},
    "Serena":   {"desc_en": "Warm, gentle young female", "desc_zh": "æ¸©æš–ã€æŸ”å’Œçš„å¹´è½»å¥³å£°", "native": "Chinese", "gender": "Female"},
    "Uncle_Fu": {"desc_en": "Seasoned male, low mellow timbre", "desc_zh": "æ²‰ç¨³ä½Žæ²‰çš„æˆç†Ÿç”·å£°", "native": "Chinese", "gender": "Male"},
    "Dylan":    {"desc_en": "Youthful Beijing male, clear natural", "desc_zh": "æ¸…æœ—è‡ªç„¶çš„åŒ—äº¬é’å¹´ç”·å£°", "native": "Chinese (Beijing)", "gender": "Male"},
    "Eric":     {"desc_en": "Lively Chengdu male, slightly husky", "desc_zh": "æ´»æ³¼ç•¥å¸¦æ²™å“‘çš„æˆéƒ½ç”·å£°", "native": "Chinese (Sichuan)", "gender": "Male"},
    "Ryan":     {"desc_en": "Dynamic male, strong rhythmic drive", "desc_zh": "åŠ¨æ„Ÿåè¶³ã€èŠ‚å¥æ„Ÿå¼ºçš„ç”·å£°", "native": "English", "gender": "Male"},
    "Aiden":    {"desc_en": "Sunny American male, clear midrange", "desc_zh": "é˜³å…‰ç¾Žå¼ç”·å£°ã€ä¸­é¢‘æ¸…æ™°", "native": "English", "gender": "Male"},
    "Ono_Anna": {"desc_en": "Playful Japanese female, light nimble", "desc_zh": "ä¿çš®è½»ç›ˆçš„æ—¥è¯­å¥³å£°", "native": "Japanese", "gender": "Female"},
    "Sohee":    {"desc_en": "Warm Korean female, rich emotion", "desc_zh": "æ¸©æš–å¯Œæœ‰æƒ…æ„Ÿçš„éŸ©è¯­å¥³å£°", "native": "Korean", "gender": "Female"},
}

SAMPLE_TEXTS = {
    "Chinese": "å…¶å®žæˆ‘çœŸçš„æœ‰å‘çŽ°ï¼Œæˆ‘æ˜¯ä¸€ä¸ªç‰¹åˆ«å–„äºŽè§‚å¯Ÿåˆ«äººæƒ…ç»ªçš„äººã€‚",
    "English": "Then by the end of the movie, when Dorothy clicks her heels and says, there's no place like home, I got a little bit teary.",
    "Japanese": "ã‚„ã°ã„ã€æ˜Žæ—¥ã®ãƒ—ãƒ¬ã‚¼ãƒ³è³‡æ–™ã¾ã å®Œæˆã—ã¦ãªã„â€¦ åŠ©ã‘ã¦ï¼",
    "Korean": "ì•¼, ì˜¤ëŠ˜ ì ì‹¬ì— ë­ ë¨¹ì„ì§€ ìƒê°í•´ ë´¤ì–´? ê·¼ì²˜ì— ìƒˆë¡œ ìƒê¸´ ë¶„ì‹ì§‘ ì–´ë•Œ?",
    "German": "Ich habe heute Morgen einen wunderschÃ¶nen Sonnenaufgang gesehen.",
    "French": "La vie est belle quand on prend le temps de regarder autour de soi.",
    "Russian": "Ð¡ÐµÐ³Ð¾Ð´Ð½Ñ Ð¿Ñ€ÐµÐºÑ€Ð°ÑÐ½Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð³ÑƒÐ»ÐºÐ¸ Ð² Ð¿Ð°Ñ€ÐºÐµ.",
    "Portuguese": "A vida Ã© uma jornada, nÃ£o um destino.",
    "Spanish": "La mÃºsica es el lenguaje universal de la humanidad.",
    "Italian": "La bellezza salverÃ  il mondo, diceva Dostoevskij.",
}

INSTRUCT_EXAMPLES = {
    "en": ["Speak with a very sad and tearful voice", "Very happy and excited", "Speaking at an extremely slow pace",
           "Whisper softly", "Speak with authority and confidence", "In a playful, teasing tone"],
    "zh": ["ç”¨ç‰¹åˆ«æ„¤æ€’çš„è¯­æ°”è¯´", "ç”¨ç‰¹åˆ«ä¼¤å¿ƒçš„è¯­æ°”è¯´", "è¯·ç‰¹åˆ«å°å£°çš„æ‚„æ‚„è¯´", "ç”¨å¼€å¿ƒæ„‰æ‚¦çš„è¯­æ°”",
           "è¯­é€Ÿå¾ˆå¿«åœ°è¯´", "ç”¨ä½Žæ²‰çš„å£°éŸ³è¯´"],
}

VOICE_DESIGN_EXAMPLES = [
    {"label": "ðŸŽ€ Cute Loli (èèŽ‰)", "instruct": "ä½“çŽ°æ’’å¨‡ç¨šå«©çš„èèŽ‰å¥³å£°ï¼ŒéŸ³è°ƒåé«˜ä¸”èµ·ä¼æ˜Žæ˜¾ï¼Œè¥é€ å‡ºé»äººã€åšä½œåˆåˆ»æ„å–èŒçš„å¬è§‰æ•ˆæžœã€‚",
     "text": "å“¥å“¥ï¼Œä½ å›žæ¥å•¦ï¼Œäººå®¶ç­‰äº†ä½ å¥½ä¹…å¥½ä¹…äº†ï¼Œè¦æŠ±æŠ±ï¼", "lang": "Chinese"},
    {"label": "ðŸŽ­ Sarcastic Teen", "instruct": "Speak as a sarcastic, assertive teenage girl: crisp enunciation, controlled volume, with vocal emphasis that conveys disdain.",
     "text": "Blah, blah, blah. We're all very fascinated, but we'd like to get paid.", "lang": "English"},
    {"label": "ðŸ‘‘ Imperial Drama (å®«å»·)", "instruct": "å±•çŽ°å‡ºæ‚²è‹¦æ²™å“‘çš„å£°éŸ³è´¨æ„Ÿ,è¯­é€Ÿåæ…¢,æƒ…ç»ªæµ“çƒˆä¸”å¸¦æœ‰å“­è…”,ä»¥æ ‡å‡†æ™®é€šè¯ç¼“æ…¢è¯‰è¯´,æƒ…æ„Ÿå¼ºçƒˆ,è¯­è°ƒå“€æ€¨é«˜äº¢ã€‚",
     "text": "çš‡ä¸Šå•Šï¼è‡£å¦¾ä¸€ç‰‡çœŸå¿ƒå¯æ˜­æ—¥æœˆï¼Œä¸ºä½•æ‚¨ç«Ÿä¿¡é‚£æ¯’å¦‡è°—è¨€ï¼Œå°†æˆ‘æ‰“å…¥å†·å®«ï¼Ÿ", "lang": "Chinese"},
    {"label": "ðŸŽ™ï¸ Announcer", "instruct": "A bright, agile male voice with a natural upward lift, delivering lines at a brisk, energetic pace. Pitch leans high, volume projects clearly.",
     "text": "Coming up next, the moment you've all been waiting for! Stay tuned, we'll be right back after this break!", "lang": "English"},
    {"label": "ðŸ˜¢ Sad Narrator (æ‚²ä¼¤)", "instruct": "ä»¥æžåº¦æ‚²ä¼¤ã€å¸¦ç€æ˜Žæ˜¾å“­è…”çš„è¯­æ°”ï¼Œç”¨è¾ƒå°çš„éŸ³é‡ç¼“ç¼“è¯‰è¯´ï¼Œè¯­é€Ÿç¼“æ…¢ï¼Œå£°éŸ³é¢¤æŠ–è€ŒåŽ‹æŠ‘ã€‚",
     "text": "æœ‰äº›äº‹ï¼Œåªè¦å›½å®¶éœ€è¦ï¼Œå°±å¾—æœ‰äººæ‰›èµ·æ¥ã€‚æˆ‘ä»¬é‚£ä¸€ä»£äººï¼Œæ˜¯èƒŒç€æ³¥åœŸé“ºè·¯çš„ã€‚", "lang": "Chinese"},
    {"label": "ðŸ—£ï¸ Confident Male", "instruct": "gender: Male. pitch: Low. speed: Deliberate pace. volume: Loud. emotion: Commanding. tone: Authoritative. personality: Confident.",
     "text": "Older gentleman, 110, maybe 111 years old, sort of a surly Elvis thing happening with him.", "lang": "English"},
]


# â”€â”€ i18n â”€â”€
I18N = {
    "en": {
        "title": "ðŸ—£ï¸ Qwen3-TTS", "subtitle": "All-in-One Text-to-Speech: Custom Voice Â· Voice Design Â· Voice Clone",
        "custom_voice": "ðŸŽ¤ Custom Voice", "voice_design": "ðŸŽ¨ Voice Design", "voice_clone": "ðŸ”Š Voice Clone",
        "text": "Text to synthesize", "lang": "Language", "speaker": "Speaker", "instruct": "Instruction (optional)",
        "ref_audio": "Reference Audio (3s+ recommended)", "ref_text": "Reference Text (transcript of ref audio)",
        "generate": "ðŸŽµ Generate", "advanced": "âš™ï¸ Advanced Settings", "gpu_status": "GPU Status",
        "offload": "ðŸ”» Offload GPU", "refresh": "ðŸ”„ Refresh", "xvec": "X-Vector Only (no ref text needed, lower quality)",
        "status": "Status", "output": "Output Audio", "save_voice": "ðŸ’¾ Save Voice Prompt",
        "load_voice": "ðŸ“‚ Load Voice Prompt", "voice_file": "Voice Prompt File (.pt)",
        "desc_cv": "Generate speech with 9 preset premium voices. Add instructions to control emotion, speed, tone, and style.",
        "desc_vd": "Design a completely new voice by describing its characteristics. Supports acoustic attributes, persona, age, emotion control.",
        "desc_vc": "Clone any voice from a 3-second audio clip. Supports cross-lingual cloning across 10 languages.",
        "sample_text": "ðŸ“ Sample Text", "instruct_examples": "ðŸ’¡ Instruction Examples",
        "design_presets": "ðŸŽ­ Design Presets", "speaker_info": "Speaker Info",
        "disclaimer": "âš ï¸ AI-generated audio for demo purposes only. Not for impersonation or illegal use.",
    },
    "zh-CN": {
        "title": "ðŸ—£ï¸ Qwen3-TTS è¯­éŸ³åˆæˆ", "subtitle": "ä¸€ç«™å¼è¯­éŸ³åˆæˆï¼šè‡ªå®šä¹‰è¯­éŸ³ Â· è¯­éŸ³è®¾è®¡ Â· è¯­éŸ³å…‹éš†",
        "custom_voice": "ðŸŽ¤ è‡ªå®šä¹‰è¯­éŸ³", "voice_design": "ðŸŽ¨ è¯­éŸ³è®¾è®¡", "voice_clone": "ðŸ”Š è¯­éŸ³å…‹éš†",
        "text": "åˆæˆæ–‡æœ¬", "lang": "è¯­è¨€", "speaker": "è¯´è¯äºº", "instruct": "æŒ‡ä»¤ï¼ˆå¯é€‰ï¼‰",
        "ref_audio": "å‚è€ƒéŸ³é¢‘ï¼ˆå»ºè®®3ç§’ä»¥ä¸Šï¼‰", "ref_text": "å‚è€ƒæ–‡æœ¬ï¼ˆå‚è€ƒéŸ³é¢‘çš„æ–‡å­—å†…å®¹ï¼‰",
        "generate": "ðŸŽµ ç”Ÿæˆ", "advanced": "âš™ï¸ é«˜çº§è®¾ç½®", "gpu_status": "GPU çŠ¶æ€",
        "offload": "ðŸ”» é‡Šæ”¾æ˜¾å­˜", "refresh": "ðŸ”„ åˆ·æ–°", "xvec": "ä»…å£°çº¹æ¨¡å¼ï¼ˆæ— éœ€å‚è€ƒæ–‡æœ¬ï¼Œè´¨é‡è¾ƒä½Žï¼‰",
        "status": "çŠ¶æ€", "output": "è¾“å‡ºéŸ³é¢‘", "save_voice": "ðŸ’¾ ä¿å­˜éŸ³è‰²", "load_voice": "ðŸ“‚ åŠ è½½éŸ³è‰²",
        "voice_file": "éŸ³è‰²æ–‡ä»¶ (.pt)",
        "desc_cv": "ä½¿ç”¨9ç§é¢„è®¾é«˜å“è´¨å£°éŸ³ç”Ÿæˆè¯­éŸ³ã€‚å¯æ·»åŠ æŒ‡ä»¤æŽ§åˆ¶æƒ…æ„Ÿã€è¯­é€Ÿã€è¯­è°ƒå’Œé£Žæ ¼ã€‚",
        "desc_vd": "ç”¨è‡ªç„¶è¯­è¨€æè¿°å£°éŸ³ç‰¹å¾æ¥è®¾è®¡å…¨æ–°å£°éŸ³ã€‚æ”¯æŒå£°å­¦å±žæ€§ã€äººè®¾ã€å¹´é¾„ã€æƒ…æ„ŸæŽ§åˆ¶ã€‚",
        "desc_vc": "ä»Ž3ç§’éŸ³é¢‘ç‰‡æ®µå…‹éš†ä»»æ„å£°éŸ³ã€‚æ”¯æŒ10ç§è¯­è¨€çš„è·¨è¯­è¨€å…‹éš†ã€‚",
        "sample_text": "ðŸ“ ç¤ºä¾‹æ–‡æœ¬", "instruct_examples": "ðŸ’¡ æŒ‡ä»¤ç¤ºä¾‹",
        "design_presets": "ðŸŽ­ è®¾è®¡é¢„è®¾", "speaker_info": "è¯´è¯äººä¿¡æ¯",
        "disclaimer": "âš ï¸ AIç”ŸæˆéŸ³é¢‘ä»…ä¾›æ¼”ç¤ºï¼Œç¦æ­¢ç”¨äºŽå†’å……ä»–äººæˆ–éžæ³•ç”¨é€”ã€‚",
    },
    "zh-TW": {
        "title": "ðŸ—£ï¸ Qwen3-TTS èªžéŸ³åˆæˆ", "subtitle": "ä¸€ç«™å¼èªžéŸ³åˆæˆï¼šè‡ªè¨‚èªžéŸ³ Â· èªžéŸ³è¨­è¨ˆ Â· èªžéŸ³è¤‡è£½",
        "custom_voice": "ðŸŽ¤ è‡ªè¨‚èªžéŸ³", "voice_design": "ðŸŽ¨ èªžéŸ³è¨­è¨ˆ", "voice_clone": "ðŸ”Š èªžéŸ³è¤‡è£½",
        "text": "åˆæˆæ–‡å­—", "lang": "èªžè¨€", "speaker": "èªªè©±äºº", "instruct": "æŒ‡ä»¤ï¼ˆé¸å¡«ï¼‰",
        "ref_audio": "åƒè€ƒéŸ³è¨Šï¼ˆå»ºè­°3ç§’ä»¥ä¸Šï¼‰", "ref_text": "åƒè€ƒæ–‡å­—ï¼ˆåƒè€ƒéŸ³è¨Šçš„æ–‡å­—å…§å®¹ï¼‰",
        "generate": "ðŸŽµ ç”Ÿæˆ", "advanced": "âš™ï¸ é€²éšŽè¨­å®š", "gpu_status": "GPU ç‹€æ…‹",
        "offload": "ðŸ”» é‡‹æ”¾é¡¯å­˜", "refresh": "ðŸ”„ é‡æ–°æ•´ç†", "xvec": "åƒ…è²ç´‹æ¨¡å¼ï¼ˆç„¡éœ€åƒè€ƒæ–‡å­—ï¼Œå“è³ªè¼ƒä½Žï¼‰",
        "status": "ç‹€æ…‹", "output": "è¼¸å‡ºéŸ³è¨Š", "save_voice": "ðŸ’¾ å„²å­˜éŸ³è‰²", "load_voice": "ðŸ“‚ è¼‰å…¥éŸ³è‰²",
        "voice_file": "éŸ³è‰²æª”æ¡ˆ (.pt)",
        "desc_cv": "ä½¿ç”¨9ç¨®é è¨­é«˜å“è³ªè²éŸ³ç”ŸæˆèªžéŸ³ã€‚å¯æ·»åŠ æŒ‡ä»¤æŽ§åˆ¶æƒ…æ„Ÿã€èªžé€Ÿã€èªžèª¿å’Œé¢¨æ ¼ã€‚",
        "desc_vd": "ç”¨è‡ªç„¶èªžè¨€æè¿°è²éŸ³ç‰¹å¾µä¾†è¨­è¨ˆå…¨æ–°è²éŸ³ã€‚æ”¯æ´è²å­¸å±¬æ€§ã€äººè¨­ã€å¹´é½¡ã€æƒ…æ„ŸæŽ§åˆ¶ã€‚",
        "desc_vc": "å¾ž3ç§’éŸ³è¨Šç‰‡æ®µè¤‡è£½ä»»æ„è²éŸ³ã€‚æ”¯æ´10ç¨®èªžè¨€çš„è·¨èªžè¨€è¤‡è£½ã€‚",
        "sample_text": "ðŸ“ ç¯„ä¾‹æ–‡å­—", "instruct_examples": "ðŸ’¡ æŒ‡ä»¤ç¯„ä¾‹",
        "design_presets": "ðŸŽ­ è¨­è¨ˆé è¨­", "speaker_info": "èªªè©±äººè³‡è¨Š",
        "disclaimer": "âš ï¸ AIç”ŸæˆéŸ³è¨Šåƒ…ä¾›æ¼”ç¤ºï¼Œç¦æ­¢ç”¨æ–¼å†’å……ä»–äººæˆ–éžæ³•ç”¨é€”ã€‚",
    },
    "ja": {
        "title": "ðŸ—£ï¸ Qwen3-TTS éŸ³å£°åˆæˆ", "subtitle": "ã‚ªãƒ¼ãƒ«ã‚¤ãƒ³ãƒ¯ãƒ³éŸ³å£°åˆæˆï¼šã‚«ã‚¹ã‚¿ãƒ éŸ³å£°ãƒ»éŸ³å£°ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ»éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ³",
        "custom_voice": "ðŸŽ¤ ã‚«ã‚¹ã‚¿ãƒ éŸ³å£°", "voice_design": "ðŸŽ¨ éŸ³å£°ãƒ‡ã‚¶ã‚¤ãƒ³", "voice_clone": "ðŸ”Š éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ³",
        "text": "åˆæˆãƒ†ã‚­ã‚¹ãƒˆ", "lang": "è¨€èªž", "speaker": "è©±è€…", "instruct": "æŒ‡ç¤ºï¼ˆä»»æ„ï¼‰",
        "ref_audio": "å‚ç…§éŸ³å£°ï¼ˆ3ç§’ä»¥ä¸ŠæŽ¨å¥¨ï¼‰", "ref_text": "å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå‚ç…§éŸ³å£°ã®æ›¸ãèµ·ã“ã—ï¼‰",
        "generate": "ðŸŽµ ç”Ÿæˆ", "advanced": "âš™ï¸ è©³ç´°è¨­å®š", "gpu_status": "GPUçŠ¶æ…‹",
        "offload": "ðŸ”» GPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰", "refresh": "ðŸ”„ æ›´æ–°", "xvec": "X-Vectorã®ã¿ï¼ˆå‚ç…§ãƒ†ã‚­ã‚¹ãƒˆä¸è¦ã€å“è³ªä½Žä¸‹ï¼‰",
        "status": "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", "output": "å‡ºåŠ›éŸ³å£°", "save_voice": "ðŸ’¾ éŸ³è‰²ä¿å­˜", "load_voice": "ðŸ“‚ éŸ³è‰²èª­è¾¼",
        "voice_file": "éŸ³è‰²ãƒ•ã‚¡ã‚¤ãƒ« (.pt)",
        "desc_cv": "9ç¨®ã®ãƒ—ãƒªã‚»ãƒƒãƒˆéŸ³å£°ã§éŸ³å£°ç”Ÿæˆã€‚æŒ‡ç¤ºã§æ„Ÿæƒ…ãƒ»é€Ÿåº¦ãƒ»ãƒˆãƒ¼ãƒ³ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ã‚’åˆ¶å¾¡ã€‚",
        "desc_vd": "è‡ªç„¶è¨€èªžã§éŸ³å£°ç‰¹æ€§ã‚’è¨˜è¿°ã—ã¦æ–°ã—ã„éŸ³å£°ã‚’ãƒ‡ã‚¶ã‚¤ãƒ³ã€‚éŸ³éŸ¿å±žæ€§ãƒ»ãƒšãƒ«ã‚½ãƒŠãƒ»å¹´é½¢ãƒ»æ„Ÿæƒ…åˆ¶å¾¡å¯¾å¿œã€‚",
        "desc_vc": "3ç§’ã®éŸ³å£°ã‚¯ãƒªãƒƒãƒ—ã‹ã‚‰éŸ³å£°ã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã€‚10è¨€èªžã®å¤šè¨€èªžã‚¯ãƒ­ãƒ¼ãƒ³å¯¾å¿œã€‚",
        "sample_text": "ðŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ", "instruct_examples": "ðŸ’¡ æŒ‡ç¤ºä¾‹",
        "design_presets": "ðŸŽ­ ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ—ãƒªã‚»ãƒƒãƒˆ", "speaker_info": "è©±è€…æƒ…å ±",
        "disclaimer": "âš ï¸ AIç”ŸæˆéŸ³å£°ã¯ãƒ‡ãƒ¢ç›®çš„ã®ã¿ã€‚ãªã‚Šã™ã¾ã—ã‚„é•æ³•ä½¿ç”¨ã¯ç¦æ­¢ã€‚",
    },
}


# â”€â”€ GPU Manager â”€â”€
class GPUManager:
    def __init__(self):
        self.model = None
        self.model_type = None
        self.tokenizer = None
        self.last_use = 0.0
        self._lock = asyncio.Lock()
        self._task = None

    async def start(self):
        self._task = asyncio.create_task(self._idle_loop())

    async def stop(self):
        if self._task:
            self._task.cancel()

    async def _idle_loop(self):
        while True:
            await asyncio.sleep(30)
            async with self._lock:
                if self.model and time.time() - self.last_use > GPU_IDLE_TIMEOUT:
                    logger.info("Auto-offloading model (idle timeout)")
                    self._unload()

    def _unload(self):
        if self.model:
            del self.model
            self.model = None
            self.model_type = None
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None
        torch.cuda.empty_cache()

    async def load(self, model_type: str):
        async with self._lock:
            self.last_use = time.time()
            if self.model and self.model_type == model_type:
                return self.model
            self._unload()
            name = MODEL_MAP[model_type]
            logger.info(f"Loading model: {name}")
            from qwen_tts import Qwen3TTSModel
            self.model = Qwen3TTSModel.from_pretrained(
                name, device_map=CUDA_DEVICE,
                dtype=torch.bfloat16, attn_implementation="flash_attention_2",
            )
            self.model_type = model_type
            logger.info("Model loaded")
            return self.model

    async def load_tokenizer(self):
        async with self._lock:
            self.last_use = time.time()
            if self.tokenizer:
                return self.tokenizer
            from qwen_tts import Qwen3TTSTokenizer
            logger.info(f"Loading tokenizer: {TOKENIZER_PATH}")
            self.tokenizer = Qwen3TTSTokenizer.from_pretrained(TOKENIZER_PATH, device_map=CUDA_DEVICE)
            return self.tokenizer

    async def offload(self):
        async with self._lock:
            self._unload()

    async def status(self):
        gpu_info = {}
        try:
            if torch.cuda.is_available():
                idx = int(CUDA_DEVICE.split(":")[-1]) if ":" in CUDA_DEVICE else 0
                gpu_info = {
                    "gpu_name": torch.cuda.get_device_name(idx),
                    "memory_allocated_mb": round(torch.cuda.memory_allocated(idx) / 1048576),
                    "memory_reserved_mb": round(torch.cuda.memory_reserved(idx) / 1048576),
                }
        except Exception:
            pass
        return {
            "loaded": self.model is not None,
            "model_type": self.model_type,
            "model_name": MODEL_MAP.get(self.model_type, ""),
            "idle_seconds": round(time.time() - self.last_use, 1) if self.last_use else None,
            **gpu_info,
        }


gpu = GPUManager()


# â”€â”€ Helpers â”€â”€
def wav_bytes(audio: np.ndarray, sr: int) -> bytes:
    buf = io.BytesIO()
    sf.write(buf, audio, sr, format="WAV")
    buf.seek(0)
    return buf.getvalue()


def gen_kwargs(do_sample=True, top_k=50, top_p=0.9, temperature=1.0, repetition_penalty=1.05,
               max_new_tokens=2048, subtalker_top_k=50, subtalker_top_p=0.9, subtalker_temperature=1.0):
    return dict(do_sample=do_sample, top_k=top_k, top_p=top_p, temperature=temperature,
                repetition_penalty=repetition_penalty, max_new_tokens=max_new_tokens,
                subtalker_top_k=subtalker_top_k, subtalker_top_p=subtalker_top_p,
                subtalker_temperature=subtalker_temperature)


def normalize_audio(wav):
    x = np.asarray(wav, dtype=np.float32)
    if x.ndim > 1:
        x = np.mean(x, axis=-1)
    m = np.max(np.abs(x)) if x.size else 0.0
    if m > 1.0 + 1e-6:
        x = x / (m + 1e-12)
    return np.clip(x, -1.0, 1.0).astype(np.float32)


def timed_wav_response(audio, sr, t_load, t_gen, filename="output.wav"):
    """Return WAV StreamingResponse with timing headers."""
    audio_dur = len(audio) / sr
    return StreamingResponse(
        io.BytesIO(wav_bytes(audio, sr)), media_type="audio/wav",
        headers={
            "Content-Disposition": f"attachment; filename={filename}",
            "X-Time-Load": f"{t_load:.3f}", "X-Time-Gen": f"{t_gen:.3f}",
            "X-Time-Total": f"{t_load + t_gen:.3f}", "X-Audio-Duration": f"{audio_dur:.3f}",
        })


# â”€â”€ FastAPI â”€â”€
@asynccontextmanager
async def lifespan(_app):
    await gpu.start()
    yield
    await gpu.stop()

app = FastAPI(title="Qwen3-TTS API", version="2.0.0",
              description="All-in-One TTS: Custom Voice / Voice Design / Voice Clone / Tokenizer", lifespan=lifespan)

from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"],
                   expose_headers=["X-Time-Load","X-Time-Gen","X-Time-Total","X-Audio-Duration",
                                   "X-Sample-Rate","X-Audio-Format","X-Audio-Channels"])


@app.get("/health")
async def health():
    return {"status": "ok", "service": "qwen3-tts", "version": "2.0.0"}


@app.get("/api/gpu-status")
async def api_gpu_status():
    return await gpu.status()


@app.post("/api/gpu-offload")
async def api_gpu_offload():
    await gpu.offload()
    return {"message": "GPU offloaded"}


@app.get("/api/speakers")
async def api_speakers():
    return {"speakers": SPEAKERS, "details": SPEAKER_INFO}


@app.get("/api/languages")
async def api_languages():
    return {"languages": LANGUAGES}


@app.get("/api/models")
async def api_models():
    return {"models": MODEL_MAP, "current": gpu.model_type}


@app.get("/api/sample-texts")
async def api_sample_texts():
    return SAMPLE_TEXTS


class CustomVoiceReq(BaseModel):
    text: str
    language: str = "Auto"
    speaker: str = "Vivian"
    instruct: str = ""
    do_sample: bool = True
    top_k: int = 50
    top_p: float = 0.9
    temperature: float = 1.0
    repetition_penalty: float = 1.05
    max_new_tokens: int = 2048
    subtalker_top_k: int = 50
    subtalker_top_p: float = 0.9
    subtalker_temperature: float = 1.0


class VoiceDesignReq(BaseModel):
    text: str
    language: str = "Auto"
    instruct: str = ""
    do_sample: bool = True
    top_k: int = 50
    top_p: float = 0.9
    temperature: float = 1.0
    repetition_penalty: float = 1.05
    max_new_tokens: int = 2048
    subtalker_top_k: int = 50
    subtalker_top_p: float = 0.9
    subtalker_temperature: float = 1.0


@app.post("/api/tts/custom-voice")
async def api_custom_voice(req: CustomVoiceReq):
    try:
        t0 = time.time()
        model = await gpu.load("custom_voice")
        t_load = time.time() - t0
        t1 = time.time()
        wavs, sr = model.generate_custom_voice(
            text=req.text, language=req.language, speaker=req.speaker, instruct=req.instruct or None,
            **gen_kwargs(req.do_sample, req.top_k, req.top_p, req.temperature,
                         req.repetition_penalty, req.max_new_tokens, req.subtalker_top_k,
                         req.subtalker_top_p, req.subtalker_temperature))
        t_gen = time.time() - t1
        return timed_wav_response(wavs[0], sr, t_load, t_gen, f"cv_{req.speaker}_{req.language}.wav")
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.exception("custom-voice error")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/tts/voice-design")
async def api_voice_design(req: VoiceDesignReq):
    try:
        t0 = time.time()
        model = await gpu.load("voice_design")
        t_load = time.time() - t0
        t1 = time.time()
        wavs, sr = model.generate_voice_design(
            text=req.text, language=req.language, instruct=req.instruct,
            **gen_kwargs(req.do_sample, req.top_k, req.top_p, req.temperature,
                         req.repetition_penalty, req.max_new_tokens, req.subtalker_top_k,
                         req.subtalker_top_p, req.subtalker_temperature))
        t_gen = time.time() - t1
        return timed_wav_response(wavs[0], sr, t_load, t_gen, f"vd_{req.language}.wav")
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.exception("voice-design error")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/tts/voice-clone")
async def api_voice_clone(
    text: str = Form(...), language: str = Form("Auto"), ref_text: str = Form(""),
    x_vector_only_mode: bool = Form(False), do_sample: bool = Form(True),
    top_k: int = Form(50), top_p: float = Form(0.9), temperature: float = Form(1.0),
    repetition_penalty: float = Form(1.05), max_new_tokens: int = Form(2048),
    subtalker_top_k: int = Form(50), subtalker_top_p: float = Form(0.9),
    subtalker_temperature: float = Form(1.0), ref_audio: UploadFile = File(...),
):
    try:
        content = await ref_audio.read()
        audio_np, audio_sr = sf.read(io.BytesIO(content), dtype="float32")
        audio_np = normalize_audio(audio_np)
        t0 = time.time()
        model = await gpu.load("voice_clone")
        t_load = time.time() - t0
        t1 = time.time()
        wavs, sr = model.generate_voice_clone(
            text=text, language=language, ref_audio=(audio_np, audio_sr),
            ref_text=ref_text or None, x_vector_only_mode=x_vector_only_mode,
            **gen_kwargs(do_sample, top_k, top_p, temperature, repetition_penalty,
                         max_new_tokens, subtalker_top_k, subtalker_top_p, subtalker_temperature))
        t_gen = time.time() - t1
        return timed_wav_response(wavs[0], sr, t_load, t_gen, "vc_clone.wav")
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.exception("voice-clone error")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/voice-prompt/save")
async def api_save_voice_prompt(
    ref_text: str = Form(""), x_vector_only_mode: bool = Form(False),
    ref_audio: UploadFile = File(...),
):
    """Save a reusable voice clone prompt from reference audio."""
    try:
        content = await ref_audio.read()
        audio_np, audio_sr = sf.read(io.BytesIO(content), dtype="float32")
        audio_np = normalize_audio(audio_np)
        model = await gpu.load("voice_clone")
        items = model.create_voice_clone_prompt(
            ref_audio=(audio_np, audio_sr),
            ref_text=ref_text.strip() or None,
            x_vector_only_mode=x_vector_only_mode,
        )
        payload = {"items": [asdict(it) for it in items]}
        ts = time.strftime("%Y%m%d_%H%M%S")
        buf = io.BytesIO()
        torch.save(payload, buf)
        buf.seek(0)
        return StreamingResponse(buf, media_type="application/octet-stream",
                                 headers={"Content-Disposition": f"attachment; filename=voice_prompt_{ts}.pt"})
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.exception("save-voice-prompt error")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/tts/voice-clone-from-prompt")
async def api_voice_clone_from_prompt(
    text: str = Form(...), language: str = Form("Auto"),
    do_sample: bool = Form(True), top_k: int = Form(50), top_p: float = Form(0.9),
    temperature: float = Form(1.0), repetition_penalty: float = Form(1.05),
    max_new_tokens: int = Form(2048), subtalker_top_k: int = Form(50),
    subtalker_top_p: float = Form(0.9), subtalker_temperature: float = Form(1.0),
    voice_prompt: UploadFile = File(...),
):
    """Generate speech using a previously saved voice prompt file."""
    try:
        from qwen_tts import VoiceClonePromptItem
        content = await voice_prompt.read()
        payload = torch.load(io.BytesIO(content), map_location="cpu", weights_only=True)
        items = []
        for d in payload["items"]:
            ref_code = d.get("ref_code")
            if ref_code is not None and not torch.is_tensor(ref_code):
                ref_code = torch.tensor(ref_code)
            ref_spk = d["ref_spk_embedding"]
            if not torch.is_tensor(ref_spk):
                ref_spk = torch.tensor(ref_spk)
            items.append(VoiceClonePromptItem(
                ref_code=ref_code, ref_spk_embedding=ref_spk,
                x_vector_only_mode=bool(d.get("x_vector_only_mode", False)),
                icl_mode=bool(d.get("icl_mode", True)),
                ref_text=d.get("ref_text"),
            ))
        t0 = time.time()
        model = await gpu.load("voice_clone")
        t_load = time.time() - t0
        t1 = time.time()
        wavs, sr = model.generate_voice_clone(
            text=text, language=language, voice_clone_prompt=items,
            **gen_kwargs(do_sample, top_k, top_p, temperature, repetition_penalty,
                         max_new_tokens, subtalker_top_k, subtalker_top_p, subtalker_temperature))
        t_gen = time.time() - t1
        return timed_wav_response(wavs[0], sr, t_load, t_gen, "vc_from_prompt.wav")
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.exception("voice-clone-from-prompt error")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/tokenizer/encode")
async def api_tokenizer_encode(ref_audio: UploadFile = File(...)):
    """Encode audio to speech tokens using Qwen3-TTS-Tokenizer-12Hz."""
    try:
        content = await ref_audio.read()
        audio_np, audio_sr = sf.read(io.BytesIO(content), dtype="float32")
        audio_np = normalize_audio(audio_np)
        tokenizer = await gpu.load_tokenizer()
        enc = tokenizer.encode(audio_np, sr=audio_sr)
        codes = enc.audio_codes[0].cpu().tolist()
        return {"codes": codes, "num_frames": len(codes)}
    except Exception as e:
        logger.exception("tokenizer-encode error")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/tokenizer/decode")
async def api_tokenizer_decode(codes: List[List[int]]):
    """Decode speech tokens back to audio."""
    try:
        tokenizer = await gpu.load_tokenizer()
        code_tensor = torch.tensor(codes).unsqueeze(0)
        wavs, sr = tokenizer.decode({"audio_codes": code_tensor})
        audio_dur = len(wavs[0]) / sr
        return StreamingResponse(
            io.BytesIO(wav_bytes(wavs[0], sr)), media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=decoded.wav",
                     "X-Audio-Duration": f"{audio_dur:.3f}"})
    except Exception as e:
        logger.exception("tokenizer-decode error")
        raise HTTPException(status_code=500, detail=str(e))


# â”€â”€ Streaming TTS endpoints (PCM raw audio) â”€â”€
def _pcm_stream(audio: np.ndarray, sr: int, chunk_duration: float = 0.25):
    """Yield raw PCM s16le chunks for transfer-chunked playback."""
    pcm = (np.clip(audio, -1.0, 1.0) * 32767).astype(np.int16).tobytes()
    chunk_bytes = int(sr * chunk_duration) * 2
    for i in range(0, len(pcm), chunk_bytes):
        yield pcm[i:i + chunk_bytes]


@app.post("/api/tts/custom-voice/stream")
async def api_custom_voice_stream(req: CustomVoiceReq):
    """Streaming custom voice TTS â€” returns raw PCM s16le audio chunks."""
    try:
        model = await gpu.load("custom_voice")
        wavs, sr = model.generate_custom_voice(
            text=req.text, language=req.language, speaker=req.speaker,
            instruct=req.instruct or None,
            **gen_kwargs(req.do_sample, req.top_k, req.top_p, req.temperature,
                         req.repetition_penalty, req.max_new_tokens, req.subtalker_top_k,
                         req.subtalker_top_p, req.subtalker_temperature))
        return StreamingResponse(
            _pcm_stream(wavs[0], sr),
            media_type=f"audio/pcm;rate={sr};encoding=signed-int;bits=16",
            headers={"X-Sample-Rate": str(sr), "X-Audio-Format": "pcm_s16le", "X-Audio-Channels": "1"})
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.exception("custom-voice-stream error")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/tts/voice-design/stream")
async def api_voice_design_stream(req: VoiceDesignReq):
    """Streaming voice design TTS â€” returns raw PCM s16le audio chunks."""
    try:
        model = await gpu.load("voice_design")
        wavs, sr = model.generate_voice_design(
            text=req.text, language=req.language, instruct=req.instruct,
            **gen_kwargs(req.do_sample, req.top_k, req.top_p, req.temperature,
                         req.repetition_penalty, req.max_new_tokens, req.subtalker_top_k,
                         req.subtalker_top_p, req.subtalker_temperature))
        return StreamingResponse(
            _pcm_stream(wavs[0], sr),
            media_type=f"audio/pcm;rate={sr};encoding=signed-int;bits=16",
            headers={"X-Sample-Rate": str(sr), "X-Audio-Format": "pcm_s16le", "X-Audio-Channels": "1"})
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.exception("voice-design-stream error")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/tts/voice-clone/stream")
async def api_voice_clone_stream(
    text: str = Form(...), language: str = Form("Auto"), ref_text: str = Form(""),
    x_vector_only_mode: bool = Form(False), do_sample: bool = Form(True),
    top_k: int = Form(50), top_p: float = Form(0.9), temperature: float = Form(1.0),
    repetition_penalty: float = Form(1.05), max_new_tokens: int = Form(2048),
    subtalker_top_k: int = Form(50), subtalker_top_p: float = Form(0.9),
    subtalker_temperature: float = Form(1.0), ref_audio: UploadFile = File(...),
):
    """Streaming voice clone TTS â€” returns raw PCM s16le audio chunks."""
    try:
        content = await ref_audio.read()
        audio_np, audio_sr = sf.read(io.BytesIO(content), dtype="float32")
        audio_np = normalize_audio(audio_np)
        model = await gpu.load("voice_clone")
        wavs, sr = model.generate_voice_clone(
            text=text, language=language, ref_audio=(audio_np, audio_sr),
            ref_text=ref_text or None, x_vector_only_mode=x_vector_only_mode,
            **gen_kwargs(do_sample, top_k, top_p, temperature, repetition_penalty,
                         max_new_tokens, subtalker_top_k, subtalker_top_p, subtalker_temperature))
        return StreamingResponse(
            _pcm_stream(wavs[0], sr),
            media_type=f"audio/pcm;rate={sr};encoding=signed-int;bits=16",
            headers={"X-Sample-Rate": str(sr), "X-Audio-Format": "pcm_s16le", "X-Audio-Channels": "1"})
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.exception("voice-clone-stream error")
        raise HTTPException(status_code=500, detail=str(e))


# â”€â”€ HTML UI â”€â”€
_UI_TEMPLATE = None

def _get_ui_html():
    global _UI_TEMPLATE
    if _UI_TEMPLATE is None:
        import pathlib
        tmpl_path = pathlib.Path(__file__).parent / "ui.html"
        raw = tmpl_path.read_text(encoding="utf-8")
        replacements = {
            "{{SPEAKERS_JSON}}": json.dumps(SPEAKERS),
            "{{LANGUAGES_JSON}}": json.dumps(LANGUAGES),
            "{{SPEAKER_INFO_JSON}}": json.dumps(SPEAKER_INFO, ensure_ascii=False),
            "{{SAMPLE_TEXTS_JSON}}": json.dumps(SAMPLE_TEXTS, ensure_ascii=False),
            "{{INSTRUCT_EXAMPLES_JSON}}": json.dumps(INSTRUCT_EXAMPLES, ensure_ascii=False),
            "{{VOICE_DESIGN_EXAMPLES_JSON}}": json.dumps(VOICE_DESIGN_EXAMPLES, ensure_ascii=False),
            "{{I18N_JSON}}": json.dumps(I18N, ensure_ascii=False),
        }
        for k, v in replacements.items():
            raw = raw.replace(k, v)
        _UI_TEMPLATE = raw
    return _UI_TEMPLATE


@app.get("/", response_class=HTMLResponse)
@app.get("/ui", response_class=HTMLResponse)
async def ui_page():
    return HTMLResponse(_get_ui_html())


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=PORT)
